<!DOCTYPE html>
<meta charset="utf-8">

<html>

<style type="text/css">
body {
	font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	font-weight: 300;
	font-size: 17px;
	margin-left: auto;
	margin-right: auto;
	width: 980px;
}
h1 {
	font-weight:300;
	line-height: 1.15em;
}

h2 {
	font-size: 1.75em;
}
a:link,a:visited {
	color: #1367a7;
	text-decoration: none;
}
a:hover {
	color: #208799;
}
h1, h2, h3 {
	text-align: center;
}
h1 {
	font-size: 40px;
	font-weight: 500;
}
h2, h3 {
	font-weight: 400;
	margin: 16px 0px 4px 0px;
}
.paper-title {
	padding: 16px 0px 16px 0px;
}
section {
	margin: 32px 0px 32px 0px;
	text-align: justify;
	clear: both;
}
.col-5 {
	 width: 20%;
	 float: left;
}
.col-4 {
	 width: 25%;
	 float: left;
}
.col-2 {
	 width: 50%;
	 float: left;
}
.row, .author-row, .affil-row {
	 overflow: auto;
}
.author-row, .affil-row {
	font-size: 20px;
}
.row {
	margin: 16px 0px 16px 0px;
}
.authors {
	font-size: 18px;
}
.affil-row {
	margin-top: 16px;
}
.teaser {
	max-width: 100%;
}
.text-center {
	text-align: center;
}
.screenshot {
	width: 256px;
	border: 1px solid #ddd;
}
.screenshot-el {
	margin-bottom: 16px;
}
hr {
	height: 1px;
	border: 0;
	border-top: 1px solid #ddd;
	margin: 0;
}
.material-icons {
	vertical-align: -6px;
}
p {
	line-height: 1.25em;
}
.caption_justify {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: justify;
	margin-top: 0px;
	margin-bottom: 64px;
}
.caption {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 64px;
}
.caption_inline {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 8px;
	margin-bottom: 0px;
}
.caption_bold {
	font-size: 16px;
	/*font-style: italic;*/
	color: #666;
	text-align: center;
	margin-top: 0px;
	margin-bottom: 0px;
	font-weight: bold;
}
video {
	display: block;
	margin: auto;
}
figure {
	display: block;
	margin: auto;
	margin-top: 10px;
	margin-bottom: 10px;
}
figure {
	display: block;
	margin: auto;
	margin-top: 10px;
	margin-bottom: 10px;
}
#bibtex pre {
	font-size: 14px;
	background-color: #eee;
	padding: 16px;
}
.blue {
	color: #2c82c9;
	font-weight: bold;
}
.orange {
	color: #d35400;
	font-weight: bold;
}
.flex-row {
	display: flex;
	flex-flow: row wrap;
	justify-content: space-around;
	padding: 0;
	margin: 0;
	list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 18px;

  border-width: 0;
  outline: none;
  border-radius: 2px;

  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: auto;
  height: auto;
  font-weight: 600;
}
.paper-btn-parent {
	display: flex;
	justify-content: center;
	margin: 16px 0px;
}
.paper-btn:hover {
	opacity: 0.85;
}
.container {
	margin-left: auto;
	margin-right: auto;
	padding-left: 16px;
	padding-right: 16px;
}
.venue {
	color: #1367a7;
}
</style>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
	<title>Towards Viewpoint Robustness in Bird's Eye View Segmentation</title>
	<meta property="og:description" content="Towards Viewpoint Robustness in Bird's Eye View Segmentation"/>
	<link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:creator" content="@mmalex">
	<meta name="twitter:title" content="Towards Viewpoint Robustness in Bird's Eye View Segmentation">
	<meta name="twitter:description" content="A paper from NVIDIA which studies the problem of viewpoint robustness in autonomous vehicles, proposing the use of novel view synthesis to solve it.">
	<meta name="twitter:image" content="https://nvlabs.github.io/viewpoint-robustness/assets/twitter.jpg">
</head>

<body>
<div class="container">
	<div class="paper-title">
		<h1>Towards Viewpoint Robustness in Bird's Eye View Segmentation</h1>
	</div>

	<div id="authors">
		<div class="author-row">
            <div class="col-4 text-center"><a href="https://tzofi.github.io/">Tzofi Klinghoffer</a><sup>1,2</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~jphilion/">Jonah Philion</a><sup>2,3,4</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~wenzheng/">Wenzheng Chen</a><sup>2,3,4</sup></div>
            <div class="col-4 text-center"><a href="https://orlitany.github.io/">Or Litany</a><sup>2</sup></div>
            <div class="col-5 text-center"><a href="https://zgojcic.github.io/">Zan Gojcic</a><sup>2</sup></div>
            <div class="col-5 text-center"><a href="https://www.jsjoo.com/">Jungseock Joo</a><sup>2,5</sup></div>
            <div class="col-5 text-center"><a href="https://www.media.mit.edu/people/raskar/overview/">Ramesh Raskar</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>2,3,4</sup></div>
            <div class="col-5 text-center"><a href="https://alvarezlopezjosem.github.io/">Jose M. Alvarez</a><sup>2</sup></div>
		</div>
        <div class="author-row">
            
        <div class="affil-row">
            <div class="col-5 text-center"><sup>1</sup>MIT</a></div>
            <div class="col-5 text-center"><sup>2</sup>NVIDIA</a></div>
            <div class="col-5 text-center"><sup>3</sup>University of Toronto</div>
            <div class="col-5 text-center"><sup>4</sup>Vector Institute</div>
            <div class="col-5 text-center"><sup>5</sup>UCLA</a></div>
        </div>

		<div class="affil-row">
			<div class="venue text-center"><b><a href="https://iccv2023.thecvf.com/">International Conference on Computer Vision, 2023</a></b></div>
		</div>

		<div style="clear: both">
			<div class="paper-btn-parent">
				<a class="paper-btn" href="assets/tzofi2023view.pdf">
					<span class="material-icons"> description </span>
					Paper
				</a>
				<a class="paper-btn" href="https://drive.google.com/drive/folders/1FQGl9oHyMb7CspUBSFQvpByZD9myLync?usp=sharing">
					<span class="material-icons"> description </span>
					Data
				</a>
				<a class="paper-btn" href="https://github.com/NVlabs/viewpoint-robustness">
					<span class="material-icons"> description </span>
					Dataset Code (coming soon!)
				</a>
				<a class="paper-btn" href="">
					<span class="material-icons"> description </span>
					Video (coming soon!)
				</a>
			</div>
		</div>
	</div>

    <section id="teaser">
            <figure style="width: 100%;">
                <a href="assets/viewpoint_problem.png">
                    <img width="100%" src="assets/viewpoint_problem.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;  text-align: justify">
                   We find that the performance of state-of-the-art methods for bird’s eye view (BEV) segmentation quickly drop with small changes to viewpoint at inference. Above we see predictions from <a href="https://github.com/bradyz/cross_view_transformers">Cross View Transformers</a> trained on data from a source rig (top). The target rig pitch is reduced by 10° (bottom), leading a 17% drop in IoU. To eliminate the viewpoint robustness problem, we propose a new method for novel view synthesis and use it to transform collected data from the source to target rig automatically.
                </p>
            </figure>
    </section>

	<section id="abstract"/>
		<h2>Abstract</h2>
		<hr>
		<p>
            Autonomous vehicles (AV) require that neural networks used for perception be robust to different viewpoints if they are to be deployed across many types of vehicles without the repeated cost of data collection and labeling for each. AV companies typically focus on collecting data from diverse scenarios and locations, but not camera rig configurations, due to cost. As a result, only a small number of rig variations exist across most fleets. In this paper, we study how AV perception models are affected by changes in camera viewpoint and propose a way to scale them across vehicle types without repeated data collection and labeling. Using bird's eye view (BEV) segmentation as a motivating task, we find through extensive experiments that existing perception models are surprisingly sensitive to changes in camera viewpoint. When trained with data from one camera rig, small changes to pitch, yaw, depth, or height of the camera at inference time lead to large drops in performance. We introduce a technique for novel view synthesis and use it to transform collected data to the viewpoint of target rigs, allowing us to train BEV segmentation models for diverse target rigs without any additional data collection or labeling cost. To analyze the impact of viewpoint changes, we leverage synthetic data to mitigate other gaps (content, ISP, etc). Our approach is then trained on real data and evaluated on synthetic data, enabling evaluation on diverse target rigs. We release all data for use in future work. Our method is able to recover an average of 14.7% of the IoU that is otherwise lost when deploying to new rigs.
		</p>
	</section>

	<section id="paper">
		<h2>Paper</h2>
		<hr>
		<div class="flex-row">
			<div style="box-sizing: border-box; padding: 16px; margin: auto;">
				<a href="assets/tzofi2023view.pdf"><img class="screenshot" src="assets/paper-thumbnail.png"></a>
			</div>
			<div style="width: 60%">
				<p><b>Towards Viewpoint Robustness in Bird's Eye View Segmentation</b></p>
				<p>Tzofi Klinghoffer, Jonah Philion, Wenzheng Chen, Or Litany, Zan Gojcic, Jungseock Joo, Ramesh Raskar, Sanja Fidler, Jose Alvarez</p>

				<div><span class="material-icons"> description </span><a href="assets/tzofi2023view.pdf"> Paper preprint (PDF, 8.3 MB)</a></div>
				<div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2309.05192"> arXiv version</a></div>
				<div><span class="material-icons"> insert_comment </span><a href="assets/tzofi2023view.bib"> BibTeX</a></div>
			</div>
		</div>
	</section>

	<section id="problem"/>
		<h2>The Viewpoint Robustness Problem</h2>
		<hr>
            <figure style="width: 100%;">
                <a href="assets/problem.png">
                    <img width="100%" src="assets/problem.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;  text-align: justify">
                   We train a source BEV model using <a href="https://nv-tlabs.github.io/lift-splat-shoot/">Lift Splat Shoot</a> and <a href="https://github.com/bradyz/cross_view_transformers">Cross View Transformers</a>, denoted at point 0 on the x axis of each graph. We then test the model across different target rigs where the camera pitch, yaw, height, or pitch and height are changed, as denoted by the different points along the x axes. We also trained each model on the target rig directly and refer to this model as the ”oracle”, as it reflects the expected upper bound IoU for each viewpoint.
                </p>
            </figure>
		<p>
            To quantify the impact of training a perception model on data from one camera viewpoint (the source rig) and testing on another camera viewpoint (the target rig), we run extensive experiments in simulation. We train on data from a source rig and then vary either the pitch, yaw, height, or pitch and height together. We find that even small changes in camera viewpoint lead to significant drops in perception model accuracy.
		</p>
	</section>

	<section id="method"/>
		<h2>Method</h2>
		<hr>
            <figure style="width: 100%;">
                <a href="assets/method.png">
                    <img width="100%" src="assets/method.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;  text-align: justify">
                   Current methods for bird’s eye view (BEV) segmentation are trained on data captured from one set of camera rigs (the source rig). At inference time, these models perform well on that camera rig, but, according to our analysis, even small changes in camera viewpoint lead to large drops in BEV segmentation accuracy. Our solution is to use novel view synthesis to augment the training dataset. We find this simple solution drastically improves the robustness of BEV segmentation models to data from a target camera rig, even when no real data from the target rig is available during training.
                </p>
            </figure>
		<p>
            We propose a method for novel view synthesis for complex autonomous vehicle (AV) scenes. Our approach builds off of <a href="https://worldsheet.github.io/">Worldsheet</a>, a recent method for novel view synthesis, that uses monocular depth estimation to transform the scene into a textured mesh, which can be used to render novel views. We use LiDAR data to supervise depth estimation, automasking to improve the quality of the LiDAR depth maps, SSIM loss to improve training robustness, and apply only the  minimal loss between neighboring frames to improve performance on scenes with occlusions. More ablations are included in the paper.
		</p>
	</section>

	<section id="results"/>
		<h2>Results</h2>
		<hr>
            <figure style="width: 100%;">
                <a href="assets/results.png">
                    <img width="100%" src="assets/results.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;  text-align: justify">
                   Shown above are the novel view synthesis results (rectified) obtained with our method for novel view synthesis. We transform images from the source rig to each of the target viewpoints and then use them for BEV segmentation training.
                </p>
            </figure>
		<p>
            We find that our method is able to produce realistic changes in camera viewpoint from monocular images. We then use these images to re-train the BEV segmentation model for the target viewpoint. As a result, BEV segmentation accuracy (IoU) significantly increases, removing the viewpoint domain gap.
		</p>
	</section>

	<section id="bibtex">
		<h2>Citation</h2>
		<hr>
		<pre><code>@inproceedings{tzofi2023view,
    author = {Klinghoffer, Tzofi and Philion, Jonah and Chen, Wenzheng and Litany, Or and Gojcic, Zan
        and Joo, Jungseock and Raskar, Ramesh and Fidler, Sanja and Alvarez, Jose M},
    title = {Towards Viewpoint Robustness in Bird's Eye View Segmentation},
    booktitle = {International Conference on Computer Vision},
    year = {2023}
}

</code></pre>
	</section>

	<section id="acknowledgements">
		<h2>Acknowledgements</h2>
		<hr>
		<div class="row">
			<p>
            We thank <a href="https://scholar.harvard.edu/adegirmenci/home">Alperen Degirmenci</a> for his valuable help with AV data preparation and <a href="https://mayings.github.io/">Maying Shen</a> for her valuable support with experiments.
			</p>
		</div>
	</section>
</div>
</body>
</html>
